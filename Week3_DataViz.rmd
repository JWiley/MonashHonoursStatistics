---
title: "Data Visualization"
author: "Joshua F. Wiley/Pei Hwa Goh/Michelle Byrne"
date: "`r Sys.Date()`"
output: 
  tufte::tufte_html: 
    toc: true
    number_sections: true
---

```{r loadpackages}
options(digits = 3)

## load relevant packages
library(tufte)
library(haven)
library(data.table)
library(JWileymisc)
library(psych)
library(ggplot2)
library(ggpubr)
library(ggthemes)
library(scales)
library(ggExtra)

## turn off some notes from R in the final HTML document
knitr::opts_chunk$set(message = FALSE)

```

# Data

To start with, we'll load some data. Here we are going to use the data
from the data collection exercise (yay!). For this code to work, you need to
download the datasets from the moodle page under General and save them into the same
directory as this `R`markdown file. You also may need to properly have
an RStudio project set up, otherwise `R` may be looking in the wrong
directory.

The `read_sav()` function from the `haven` package let's you read SPSS
data files into `R` and then we use the `as.data.table()` function to
convert the datasets into data.tables named `db` for baseline and `dd`
for the daily data.

```{r loaddata}

## read in data
db <- as.data.table(read_sav("[2021] PSY4210 BL.sav")) # baseline
dd <- as.data.table(read_sav("[2021] PSY4210 DD.sav")) # daily

```

## Scoring and Reliability

When we are working with our own data, often we have to perform some
data management. For example the Positive and Negative Affect Schedule
(PANAS) has different items (adjectives) capturing words related to
positive and negative emotions. However, we do not typically analyze
these individually. The individual items are scored into two
subscales: positive and negative affect. We may use digital (e.g.,
Qualtrics, REDCap) or paper and pencil surveys, but those typically
only provide us with the scores on each item. We have to create the
subscale scores on our own. Put simply, we compute variable scores 
from item scores.

Almost always in psychology, scales/subscales are scored in one of two
ways: either the individual items are simply added together
or the individual items are averaged. Sometimes items are worded in
the opposite direction and must first be reverse coded and then
added/averaged.
Let's look at a few ways of doing this in `R`.

* The simplest approach to adding items together is to literally list
  each item variable name separated by `+` for addition. This will
  work to create a total score. However, a downside is that under this
  approach if a participant misses *any* single item, they will be
  missing on the entire subscale.
* Another approach is to use the `rowMeans()` function. This function
  allows you to perform the calculation (take the mean for a row of
  data) excluding missing data if desired, which equates to imputing
  the mean for an individual for any missing items. This is commonly
  done and if there are small amounts of missing data (e.g., if
  someone completed 18 / 20 items and only missed 2 / 20 items, is
  probably a good idea).
* If you want to deal with missing data but need a total score, you
  can use `rowMeans()` but multiply the results by the number of items
  that *should* have been completed (e.g., 10 if a scale has 10
  items).
  
  
As a rule of thumb, I recommend using `rowMeans()` and if the scale is
typically added up, then multiply. This uses the most available data
and most the time is sensible in my experience with raw data. 
  

`r margin_note("Often in R, you will see short and long ways of doing the same thing. This may seem confusing at first, but you get used to it. The reason is that there are some circumstances where you really do need the long way, but mostly the short way is quicker and easier. For example, if you have a group of friends, but only one 'Jane' in the group, you probably just say 'Jane'. However, if your friend group grows and you end up with two 'Janes' then you might call one 'Jane T' or 'new Jane' or find some other way to indicate which 'Jane' you are talking about. The same applies in R. There are usually shortcuts which are the default, but now and then you will get in a situation where you need a longer or more specific way of referring to a function or of accomplishing a specific task that cannot be managed with the short simple way.")`

Let's look at each of these for the variable perceived stress from the
baseline questionnaire of our data collection exercise. Finally, its 
standard to report the internal consistency or reliability of a scale. 
A common measure is Cronbach's alpha, which we can get by using the `alpha()` function from the `psych` package.
We do something new here by writing: `psych::alpha()` that is a long
way of typing it and tells `R` that we want to use the `alpha()`
function from the `psych` package. Mostly we don't need to do
this. However, the `ggplot2` package also has a function called 
`alpha()` which is used for the alpha transparency level in
plots. Because we are using two packages with the same function name,
`R` can get confused so we write the package name in front of the
function to be explicit about which one to use. If we were only using
the `psych` or only using the `ggplot2` package, this would not be
necessary.

`r margin_note("You might have noticed something new, we used .SD, thats also a special symbol in data.table that means, the currently selected data. Well what IS the currently selected data? Whatever rows we picked and whichever columns specified by .SDcols, which we also listed at the end. That is needed because rowMeans() expects to be given a data set, not individual variables, but we are calling it already within db, a dataset, so we need some way of referring to a subset of the dataset within the data.table and the way we do that is with .SD. Its okay if that doesn't make sense right now, just copy and paste the code and know where to change variable names is enough for this unit. More: https://cran.r-project.org/web/packages/data.table/vignettes/datatable-sd-usage.html")`

```{r scoring} 

## add items together, if missing any item, missing Stress
db[, StressADD := PSS1 + (6-PSS2r) + (6-PSS3r) + PSS4]

## Or I could recode the variables PSS2r and PSS3r first
db [, PSS2 := 6- PSS2r]
db [, PSS3 := 6- PSS3r] 

## average items 
db[, StressAVG := rowMeans(.SD, na.rm = TRUE),
   .SDcols = c("PSS1", "PSS2", "PSS3", "PSS4")]

## average items then multiply to get back to "sum" scale
db[, Stress := rowMeans(.SD, na.rm = TRUE) * 4,
   .SDcols = c("PSS1", "PSS2", "PSS3", "PSS4")]

## Let's look at how stressed people in PSY4210 are in general
summary(db$StressAVG)

## calculate Cronbach's alpha
psych::alpha(as.data.frame(db[, .(PSS1, PSS2, PSS3, PSS4)]))

## let's try with out the reverse-coded items for PSS 2 and 3
## Pay attention to the warnings provided
psych::alpha(as.data.frame(db[, .(PSS1, PSS2r, PSS3r, PSS4)]))

## Let's do the same thing with an additional 'check.keys=TRUE' option 
psych::alpha(as.data.frame(db[, .(PSS1, PSS2r, PSS3r, PSS4)]), check.keys = TRUE)

## create a categorical stress variable
db[StressAVG < 3, StrCat := "low"]
db[StressAVG >= 3, StrCat := "high"]
db[, StrCat := factor(StrCat, levels = c("low", "high"))]

## wouldn't be a bad idea to also let R know that sex and relsta are factors
db[, relsta := factor(
  relsta, levels = c(1,2,3), 
  labels = c("single", "in a committed exclusive relationship", "in a committed nonexclusive relationship"))]

db[, sex := factor(
  sex,
  levels = c(1,2),
  labels = c("male", "female"))]


``` 


## Try It - Scoring and Reliability

Now its your turn to score the some other scales in the baseline questionnaire! 
Four items from the Sternberger's Trait Anxiety scale (STAI; items STAI1-STAI4)
were used in to measure anxiety levels. The short-form of the UCLA Loneliness 
Scale (ULS-8; items ULS1-ULS8) was used to capture loneliness. The Lifespan Self-Esteem Scale (items LSE1-LSE4) was used to measure self-esteem or how one 
feels about themselves. The 10-item version of the Big Five inventory was used
to capture your openness, conscientiousness, extraversion, agreeableness and 
neuroticism (e.g. BFI_E1r needs to be reverse-coded and then added to or 
averaged with BFI_E2 to form the Extraversion score).

Calculate Cronbach's alpha measure of internal consistency reliability
for the self-esteem score.

```{r tryit_scoring}

## create a variable for self-esteem and name it SE, and others if you like! 
## Go on, have fun with this :)


## calculate the internal consistency reliability
## for self-esteem

```

# Grammar of Graphics

`ggplot2` is based on the **g**rammar of **g**raphics, a framework for
creating graphs.

The idea is that graphics or data visualization generally can be
broken down into basic low level pieces and then combined, like
language, into a final product.

Under this system, line plots and scatter plots are essentially the
same. Both have data mapped to the x and y axes.  The difference is
the plotting symbol (**ge**ometries labelled `geom`s in `R`) in
is a point or line. The data, axes, labels, titles, etc. may be
identical in both cases.

`ggplot2` also uses aesthetics, which control how geometries are
displayed. For example, the size, shape, colour, transparency level
all are **aes**thetics.

## Univariate Graphs

To begin with, we will make some simple graphs using `ggplot2`. The
first step is specifying the dataset and mapping variables to axes.
For basic, univariate plots such as a histograms, we only need to
specify the dataset and what variable is mapped to the x axis.
We can re-use this basic setup with different **geom**etries to make
different graphs.

```{r}

pb <- ggplot(data = db, aes(x = Stress))

```

`r margin_note("Histograms define equal width bins on the x axis and
count how many observations fall within each bin. Bars display these
where the width of the bar is the width of the bin and the height of
the bar is the count (frequency) of observations falling within that
range. Histograms show a univariate distribution.")`

Using our basic mapping, we can "add" a histogram geometry to view a
histogram.

`r margin_note("When you use geom_histogram() or geom_dotplot() you'll
likely get a warning that a default number of bins were used and you
should specify a better binwidth. The binwidth controls how wide the
histogram bars are or how wide the dots in the dotplot are. However,
for our purposes the default option is almost always good enough and
you can feel free to ignore this message generally, unless you really
want to change your histogram.")`

```{r, fig.width = 6, fig.height = 5, fig.cap = "A histogram in ggplot2 for stress"}

pb + geom_histogram()

## looks like we have a pretty normal distribution based on this
```


`r margin_note("Density plots attempt to provide an empirical
approximation of the probability density function (PDF) for data.
A probability density function always sums to one (i.e., if you
integrated to get the area under the curve, it would always be one).
The underlying assumption is that the observed data likely come form
some relatively smooth distribution, so typically a smoothing kernel
is used so that you see the approximate density of the data, rather
than seeing exactly where each data point falls.
Density plots show a univariate distribution.")`

We also can make a density plot, which also attempts to show the
distribution, but using a smooth density function rather than binning
the data and plotting the frequencies. Like histograms, the height
indicates the relative frequency of observations at a particular
value. Density plots are designed so that they sum to one. 

```{r, fig.width = 6, fig.height = 5, fig.cap = "A density plot for stress"}

pb + geom_density()

``` 

`r margin_note("Dot plots show the raw data. The data values are on the x
axis. If two data points would overlap, they are vertically
displaced leading to another name: stacked dot plots. They are good
for small datasets. The y axis is kind of like a discrete density. Dot
plots show a univariate distribution.")`

Another type of plot are (stacked) dotplots. These are very effective
at showing raw data for small datasets. Each dot represents one
person. If two dots would overlap, they are stacked on top of each
other. While these often are difficult to view with large datasets,
for small datasets, they provide greater precision than histograms.

```{r, fig.width = 6, fig.height = 3, fig.cap = "A dot plot for stress"}

pb + geom_dotplot()

```

Finally, we can make Q-Q plots, although these require sample values
instead of values on just the x-axis. We use the `scale()` function to
z-score the data on the fly. Finally, we add a line with an intercept
(`a`) and slope (`b`) using `geom_abline()` which is the line all the
points would fall on if they were perfectly normally distributed.
Remember: z=(xâˆ’mean)/SD

```{r, fig.width = 5, fig.height = 5, fig.cap = "A QQ plot for stress"}
# "sample" is z-scores of Stress, geom_qq makes "theoretical" Z-scores of a standard normal distribution
ggplot(db, aes(sample = scale(Stress))) +
  geom_qq() +
  geom_abline(intercept = 0, slope = 1)

```

## Checking Distributions

`r margin_note('We do not have to assume a normal distribution. The testDistribution() function supports many other types of distributions. For example this code would test whether the data followed a chi-squared distribution: </br> plot(testDistribution(db$Stress, distr = "chisq", starts = list(df = 5),
  extremevalues = "theoretical", ev.perc = .005))
</br>
The log likelihood value is outputed for each graph, and this can be used to empirically pick the better fitting distribution as whichever distribution provides the highest log likelihood for the data. We do not get into different distributions too much in this unit, but as you go on, you may find that many variables do not follow a normal distribution and there are many statistical models that do not require outcome variables to follow a normal distribution.
</br>
</br>
To see more examples, look at: http://joshuawiley.com/JWileymisc/articles/diagnostics-vignette.html
   
   ')`

We often use graphs also to visually assess assumptions, such as
normality and for outliers. Most commonly, and certainly in what you
likely learned to date, we will be assuming variables follow a normal
distribution. We can use the `testDistribution()` function combined
with `plot()` to create a plot that helps us examine both the
distribution and outliers. There are two parts to this graph. First,
it makes a density plot of the raw data as a solid black line. A dashed
blue line superimposed shows what a normal distribution would look
like with the same mean and standard deviation as the observed
data. If these two densities are close, that indicates the variable is
approximately normally distributed. The x axis labels are a five
number summary of the data, they show:

* Minimum (0th percentile)
* 1st quartile (25th percentile)
* Median (50th percentile) 
* 3rd quartile (75th percentile)
* Maximum (100th percentile)

We also get a rug plot (the little vertical lines between the axis and
the density plot) which are lines where there is raw data. That helps
see where the raw data fall.

Below the density plot is a deviates plot. This is like a QQ plot, but
rotated 45 degrees so that the line is horizontal instead of at an
angle. If the dots fall near to the line at 0, it means they fall
exactly where a theoretical normal distribution would be.

Finally, if we assume a variable follows a normal distribution, we may
use z scores to identify extreme values or outliers. Z scores make
sense when we assume a theoretical distribution, like the normal
distribution. We can have outliers identified automatically by
specifying `extremevalues = "theoretical"` and then specifying the
percentage of the theoretical distribution in each tail we consider
extreme. For example `ev.perc = .005` means that we consider any score
that is in the bottom 0.5% or top 0.5% of a normal distribution with
the mean and standard deviation we observed for our data to be an
"extreme" value and these will be identified in black while the rest
of points are a light grey. There are no fixed guidelines on what
threshold to use, many people use the top and bottom 0.1% as well
(`ev.perc = .001`).

```{r, fig.width = 5, fig.height = 5, fig.cap = "A distribution checking plot of stress"}

plot(testDistribution(db$Stress,
  extremevalues = "theoretical", ev.perc = .005))

## looks like one guy could be a deviant!
```

## Mapping Additional Variables

`r margin_note("When there are multiple univariate distributions to
view, density plots are probably one of the most efficient
ways. Histograms are difficult to view because they either are
stacked, which makes interpretation more difficult or dodged which is
visually difficult to see, or overplotted, which can hide some of the
data.")`

We can map additional variables to aesthetics such as the colour to
include more information. 
For density plots, separating by colour is easy, by adding another
variable, say `StrCat`or `sex` or `relsta` as an additional aesthetic. 
For categorical aesthetics like color, if it had not already been a factor, 
its a good idea to convert it to a factor first so `R` knows
that it is discrete and to order levels in the desired order we want,
not the default alphabetical order (unless that is what you want).


```{r, fig.width = 6, fig.height = 5, fig.cap = "Density plot coloured by sex (and others) for stress"}

ggplot(db, aes(Stress, colour = sex)) +
  geom_density() ## continuous stress scores by sex

##To remove the NAs for sex
ggplot(db[!is.na(sex)], aes(Stress, colour = sex)) +
  geom_density() ## continuous stress scores by sex

##Let's try with other categories
ggplot(db[!is.na(relsta)], aes(Stress, colour = relsta)) +
  geom_density() ## continuous stress scores by relationship status

##Let's try to see how self-esteem varies by stress categories

db[, SE:= rowMeans(.SD, na.rm = TRUE),
   .SDcols = c("LSE1", "LSE2", "LSE3", "LSE4")]

ggplot(db, aes(SE, colour = StrCat)) +
  geom_density() ## Self-esteem by high vs low stress

```

For histograms, rather than control the colour of the lines, it is
more helpful to control the fill colour. By default, overlapping bars
are stacked on top of each other. So that there is not an `NA` group
we remove anyone who is missing sex

```{r, fig.width = 6, fig.height = 5, fig.cap = "Histogram coloured by sex for stress"}

ggplot(db[!is.na(sex)], aes(Stress, fill = sex)) +
  geom_histogram()

```

Overlapping bars also can be dodged instead of stacked.

```{r, fig.width = 6, fig.height = 5, fig.cap = "Dodged histogram coloured by sex for stress"}

ggplot(db[!is.na(sex)], aes(Stress, fill = sex)) +
  geom_histogram(position = "dodge")

```

## Try It - Univariate

Using the `db` baseline dataset:

1. make a histogram for the variable: `extraversion`.
2. use `testDistribution()` to examine whether `selfesteem` follows a
   normal distribution and whether it has any extreme values.
3. Make a dotplot for: `openness` *separated* (e.g., by colour and/or fill)
   by `sex`:
   
```{r tryunivariate, error=TRUE}

## histogram code here (extraversion)

## distribution check code here (selfesteem)

## dotplot code here (openness by sex)


```

# Bivariate Graphs

We can make bivariate plots by mapping variables to both the x and
y-axis. For a scatter plot, we use point geometric objects.

```{r, fig.width = 5, fig.height = 5, fig.cap = "Scatter plot of stress and selfesteem"}

ggplot(db,
       aes(
         x = Stress,
         y = SE)) +
  geom_point()

```

We also can use lines for bivariate data. For this example, we will
calculate the average `mood` and `energy` by day in the daily dataset
and save this as a new, small dataset. 
Compared to a scatter plot, we only change point to line geometric objects.

```{r, fig.width = 6, fig.height = 5, fig.cap = "Line plot of day and mood"}

dsum <- dd[, .(
  mood = mean(dMood, na.rm = TRUE),
  energy = mean(dEnergy, na.rm = TRUE)),
  by = SurveyDay]


ggplot(dsum, aes(SurveyDay, mood)) +
  geom_line()

```

With relatively discrete x axis, we can use a barplot for bivariate
data. By default, `geom_bar()` calculates the count of observations
that occur at each x value, so if we want our values to be the actual
bar height, we set `stat = "identity"`.

```{r, fig.width = 6, fig.height = 5, fig.cap = "Bar plot of day and mood"}

ggplot(dsum, aes(SurveyDay, mood)) +
  geom_bar(stat = "identity")

```

The grammar of graphics is designed to be like sentences, where you
can add or modify easily. For example, "There is a ball." or "There is
a big, red, striped ball." are both valid sentences. So to with
graphics, we often can chain pieces together to make it more nuanced.
In `R` we just "add" more by separating each component with `+`. 
Note that most argument names (i.e., `data = `, `mapping = `) are not
strictly required. `R` will match input to the correct argument by
position, often. The two sets of code below yield the same plot. In
the first, we explicitly label all arguments, in the second we rely on
position matching. Positional matching does not always work, for
example we still must specify `size = ` because we don't always
provide input for every argument, instead relying on defaults and only
changing the specific arguments we want changed from defaults.

```{r, fig.width = 6, fig.height = 5, fig.cap = "Bar plot of day and energy"}

ggplot(dsum, aes(SurveyDay, energy)) +
  geom_bar(stat = "identity")


# Create a numeric version of Survey Day in dsum and name it nday
dsum[,nday:= as.numeric(SurveyDay)]

# Create a factor version of Survey Day in dsum and name it fday
dsum[,fday:= as.factor(SurveyDay)]

# Now let's create a pretty plot with colours! And with some lines and points.
ggplot(data = dsum,
       mapping = aes(x = nday,  y = energy)) +
  geom_bar(mapping = aes(fill = nday),
           stat = "identity") +
  geom_line(size = 2) + 
  geom_point(size = 6) 
## note the continuous colour scale when your x is continuous

# Now let's do the same thing but with the factor version of SurveyDay
ggplot(data = dsum,
       mapping = aes(x = fday,  y = energy)) +
  geom_bar(mapping = aes(fill = fday),
           stat = "identity") +
  geom_line(size = 2) + 
  geom_point(size = 6)

# Alternatively you can also present your code as follows
ggplot(dsum, aes(fday, energy)) +
  geom_bar(aes(fill = fday), stat = "identity") +
  geom_line(size = 2) + 
  geom_point(size = 6)

# If you want to be even more extra (note use of both fday and nday)
ggplot(dsum, aes(nday, energy)) +
  geom_bar(aes(fill = fday), stat = "identity") +
  geom_line(size = 5, colour="green") + 
  geom_point(size = 3, colour = "orange")

```

# Improving Data Visualization

Before we continue examining graphs, it is helpful to think a bit more
about what makes a good graph or good data visualization.

Edward Tufte and William Cleveland are two authors who have written
extensively on data visualization and how to make good graphs. There
work is well worth reading to improving understanding on how to
efficiently convey data graphically.

- Tufte: [https://www.edwardtufte.com/tufte/](https://www.edwardtufte.com/tufte/)
- Cleveland: [http://www.stat.purdue.edu/~wsc/](http://www.stat.purdue.edu/~wsc/)

A few additional pages with good discussions on making good graphs:
[Graphic Design Stack Exchange 1](https://graphicdesign.stackexchange.com/questions/35052/how-to-visualise-two-dimensional-scientific-data-points-in-a-chart-in-graysca/35062#35062)
and [Graphic Design Stack Exchange 2](https://graphicdesign.stackexchange.com/questions/36908/best-plotting-symbols-for-scientific-plots-with-multiple-datasets/57122)

One key principle that Tufte emphasises is the data to ink ratio. This
ratio is how much data is conveyed versus ink used, and Tufte argues
to try to maximize this (i.e., more data, less ink). To see this,
consider the following graph, which is a fairly standard way stats
programs (Excel, etc.) tend to make barplots.

```{r, fig.width = 6, fig.height = 5, fig.cap = "Bar plot of day and energy - Step 1"}

ggplot(dsum, aes(SurveyDay, energy)) +
  geom_bar(stat = "identity")

``` 

For starters, the borders tell us nothing. They edge the space but
convey no information. This can be cleaned up using a different
theme.

```{r, fig.width = 6, fig.height = 5, fig.cap = "Bar plot of day and energy - Step 2"}

ggplot(dsum, aes(SurveyDay, energy)) +
  geom_bar(stat = "identity") + 
  theme_pubr()

``` 

However, there are still some borders, which we can strip away with no
loss of data, but reducing the ink.

```{r, fig.width = 6, fig.height = 5, fig.cap = "Bar plot of day and energy - Step 2"}

ggplot(dsum, aes(SurveyDay, energy)) +
  geom_bar(stat = "identity") +
  theme_pubr() + 
  theme(axis.line = element_blank())

``` 

Next, think about what data are conveyed in this graph. The bars
capture two pieces of information: (1) the day and (2) the energy on
that day. The only pieces of the bars we really need are the top. The
rest of the bars take up a lot of ink, but convey no data. Points can
do this more efficiently. The chart that follows has a much higher
data-to-ink ratio as it is stripped back nearly to just the data.

```{r, fig.width = 6, fig.height = 5, fig.cap = "Dot plot of day and energy - Step 3"}

ggplot(dsum, aes(SurveyDay, energy)) +
  geom_point(size = 4) +
  theme_pubr() + 
  theme(axis.line = element_blank())

``` 

Depending on the number of data points, one may push a bit
further. Many people in practice find they are unfamiliar with these
sort of graphs and at first it can take a bit longer to read. We are
trained and used to seeing plots with "chartjunk" and low data to
ink ratios. However, a chart like this is a far more condensed display
of data and removes distractions to really highlight the raw data or
results.

```{r, fig.width = 5, fig.height = 4, fig.cap = "Dot plot of day and energy - Step 4"}

ggplot(dsum, aes(SurveyDay, energy)) +
  geom_point(size = 4) +
  geom_text(aes(y = energy + .04, label = round(energy, 2))) +
  theme_pubr() +   
  theme(axis.line = element_blank(),
        axis.ticks.x = element_blank(),
        axis.ticks.y = element_blank(),
        axis.text.y = element_blank(),
        axis.title.y = element_blank()) +
  ggtitle("Energy across days")

``` 

Another example is the popular scatter plot. By default scatter plots
already have relatively high data to ink ratios.

```{r, fig.width = 5, fig.height = 5, fig.cap = "Scatter plot - Step 1"}

ggplot(db, aes(Stress, SE)) +
  geom_point()

``` 

However, "normal" axes don't convey data, so they can be removed.

```{r, fig.width = 5, fig.height = 5, fig.cap = "Scatter plot - Step 2"}

ggplot(db, aes(Stress, SE)) +
  geom_point() +
  theme_pubr() + 
  theme(axis.line = element_blank())

``` 

If we want something *like* axes but to be more useful, we can use the
function `geom_rangeframe()` from the `ggthemes` package to put more
informative data in. Range frames add "axes" but only that go the
range of the observed data. Thus, these new axes show the minimum and
maximum of each variable.

```{r, fig.width = 5, fig.height = 5, fig.cap = "Scatter plot - Step 3"}

ggplot(db, aes(Stress, SE)) +
  geom_point() +
  theme_pubr() +   
  theme(axis.line = element_blank()) +
  geom_rangeframe() 

``` 

`r margin_note("These informative axes are created by asking 
ggplot2 to create tick marks (breaks) on the x and y axis at the 
quintiles of the variables, which is the default output from the 
quantile() function. So now instead of the default tick marks /
breaks on the axes, the breaks and numbers are:
minimum (0th percentile), </br>
25th percentile (i.e., lower quartile), </br>
50th percentile (i.e., median), </br>
75th percentile (i.e., upper quartile), </br>
maximum (i.e., 100th percentile). </br>
This provides a useful descriptive statistics on each variable in the
plot, right in the axes.")`


Finally, we can make the axis labels more informative. Instead of
presenting "pretty" numbers but that convey no data, we can pick axis
labels and breaks at meaningful points of the data.
One option is quantiles / percentiles: 0th, 25th, 50th (median), 75th
and 100th percentiles are given by default from the `quantile()`
function. Now almost every piece of ink in this figure conveys some
useful information. We can visually see the range of the `stress` and
`selfesteem` variables from the axes. We can see the median and interquartile
range as well. 

```{r, fig.width = 5, fig.height = 5, fig.cap = "Scatter plot - Step 4"}

ggplot(db, aes(Stress, SE)) +
  geom_point() +
  scale_x_continuous(breaks = as.numeric(quantile(db$Stress))) + 
  scale_y_continuous(breaks = as.numeric(quantile(db$SE))) +   
  theme_pubr() +   
  theme(axis.line = element_blank()) +
  geom_rangeframe() 

``` 

In the rest of the graphs we examine, we will try to implement this
data to ink ratio principle. It does require some additional `R` code
versus simply plotting with the defaults and often at first, you may
need to spend a bit more time explaining your graph to people in text
or in the figure legend. However, ultimately, such plots convey more
data.

# Advanced Bivariate Graphs

As with univariate graphs, we can map additional variables to
additional aesthetics. This allows us to integrate more into standard
bivariate plots. The following graph shows a simple example of
this. 

```{r, fig.width = 6, fig.height = 5, fig.cap = "Scatter plot with shapes"}

ggplot(db[!is.na(sex)], aes(Stress, SE, shape = sex)) +
  geom_point() +
  scale_x_continuous(breaks = as.numeric(quantile(db$Stress))) + 
  scale_y_continuous(breaks = as.numeric(quantile(db$SE))) +   
  theme_pubr() +   
  theme(axis.line = element_blank()) +
  geom_rangeframe() 

```

`r margin_note("There are many other shapes available. To see a list,
    go to: http://sape.inf.usi.ch/quick-reference/ggplot2/shape.")`

However, although the shapes are distinguishable, they are
visually difficult. Cleveland did some studies around shape
perception, particularly when points may be partially overlapping and
on this basis suggested other shapes. We can manually specify the
number for which shape we want applied to which value of `sex` using
the `scale_shape_manual()` function. We also use the
`name = ` argument so that instead of getting the legend labelled
`sex` it is labelled `Sex` (yes, with a capital S). 

```{r, fig.width = 6, fig.height = 5, fig.cap = "Scatter plot with shapes"}

ggplot(db[!is.na(sex)], aes(Stress, SE, shape = sex)) +
  geom_point() +
  scale_shape_manual(
    name = "Sex",
    values = c("male" = 1, "female" = 3)) +   
  scale_x_continuous(breaks = as.numeric(quantile(db$Stress))) + 
  scale_y_continuous(breaks = as.numeric(quantile(db$SE))) +   
  theme_pubr() +   
  theme(axis.line = element_blank()) +
  geom_rangeframe() 

```

## Try It - Bivariate

Make a scatter plot (points) for `extraversion` and
`conscientiousness` in the baseline data, `db`. Use the
good visualization principles we have learned. Make your own
decisions to make the scatter plot most useful to read.

```{r trybivariate, error=TRUE}

## scatter plot code here




```

# Presentation and Publication Plots

`r margin_note("Many more examples can be found online, for example: http://www.cookbook-r.com/Graphs/")`

Here we are going to put together several of the ideas learned to make
some plots that could be included in presentations or publications.
We will go through these fairly briefly and they serve largely as a
"cookbook" with some examples you may want to use yourself later.

```{r, fig.width = 6, fig.height = 5, fig.cap = "Boxplot with raw data shown"}

ggplot(dd, aes(factor(SurveyDay), dEnergy)) +
  geom_boxplot() +
  geom_jitter(colour = "lightgrey") +
  theme_pubr() +
  scale_y_continuous(
    "Daily Energy Ratings",
    breaks = as.numeric(quantile(dd$dEnergy))) +
  xlab("Days")

```

```{r, fig.width = 6, fig.height = 4, fig.cap = "Mean and 95 percent confidence interval"}

ggplot(dd, aes(factor(SurveyDay), dEnergy)) +
  stat_summary(fun.data = mean_cl_normal) +  
  theme_pubr() +
  ylab("Average (95% CI) Energy") + 
  xlab("Days")

```

```{r, fig.width = 6, fig.height = 4, fig.cap = "Chartjunk mean and 95 percent confidence interval"}

ggplot(dd, aes(factor(SurveyDay), dEnergy)) +
  stat_summary(fun.data = mean_cl_normal, geom = "bar") +
  stat_summary(fun.data = mean_cl_normal, geom = "errorbar", width = .3) +    
  theme_pubr() +
  ylab("Average (95% CI) Energy") + 
  xlab("Days")

```

```{r, fig.width = 6, fig.height = 4, fig.cap = "Mean and 95 percent confidence intervals"}

ggplot(db[!is.na(sex)], aes(sex, Stress)) +
  stat_summary(fun.data = mean_cl_normal, size = 1) +
  theme_pubr() +
  ylab("Average (95% CI) Stress") + 
  xlab("Sex")

```

```{r, fig.width = 5, fig.height = 3.5, fig.cap = "Mean and 95 percent confidence interval with data"}

ggplot(db[!is.na(sex)], aes(sex, Stress)) +
  geom_jitter(colour = "lightgrey", width = .1) +   
  stat_summary(fun.data = mean_cl_normal, size = 1) +
  theme_pubr() +
  ylab("Average (95% CI) Stress") + 
  xlab("Sex")

```

```{r, fig.width = 6, fig.height = 5, fig.cap = "Likert plot of means showing anchors"}

## view the labels for one of the ULS items
attr(db$ULS1, "labels")

## make a summarised dataset with the means and labels
sumdat <- melt(db[, .(ID,
            LackCompanionship = ULS1, LeftOut = ULS4,
            Isolated = ULS5, NoOneToTurnTo = ULS2)], id.vars = "ID")[,
  .(Mean = mean(value, na.rm = TRUE)), by = variable]
sumdat[, Never := paste0(variable, "\nNever")]
sumdat[, Often := paste0(variable, "\nOften")]

## make a likert plot
gglikert("Mean", "variable", "Never", "Often", data = sumdat,
         xlim = c(1, 5),
         title = "Average Loneliness Ratings")

```


# Summary Table

Here is a little summary of some of the functions used in this
topic. You might also enjoy this "cheatsheet" for `ggplot2`:
https://github.com/rstudio/cheatsheets/raw/master/data-visualization-2.1.pdf


| Function       | What it does                                 |
|----------------|----------------------------------------------|
| `ggplot()`     | Sets the dataset and which variables map to which aesthetics for a plot |
| `geom_boxplot()` | Adds geometric object for boxplots | 
| `geom_density()` | Adds a geometric object for density lines, a way to view a distribution |
| `geom_histogram()` | Adds a geometric object for a histogram, a way to view a distribution |
| `geom_jitter()` | Adds a points with some automatic random noise, helpful when one axis is discrete |
| `stat_summary()` | Used to automatically calculated some summary statistics on data and plot, usually means with standard errors or confidence intervals | 
| `gglikert()` | Create a likert type plot showing means typically with the scale anchors |
| `plot(testDistribution())` | Used to check whether a variable follows a specific, assumed distribution, typically a normal distribution | 
| `ylab()` | Adds a label for the y axis |
| `xlab()` | Adds a label for the x axis |

# PART 2

```{r loaddata2}

## read in data
db <- as.data.table(read_sav("[2021] PSY4210 BL.sav")) # baseline

## compute a stress variable if you haven't already done so
db [, PSS2 := 6- PSS2r]
db [, PSS3 := 6- PSS3r] 
db[, Stress := rowMeans(.SD, na.rm = TRUE) * 4,
   .SDcols = c("PSS1", "PSS2", "PSS3", "PSS4")]

## compute a self-esteem variable if you haven't already done so
db[, selfesteem:= rowMeans(.SD, na.rm = TRUE),
.SDcols = c("LSE1", "LSE2", "LSE3", "LSE4")]

## compute the personality variables if you haven't already done so
db [, BFI_N1 := 6- BFI_N1r]
db[, neuroticism:= rowMeans(.SD, na.rm = TRUE),
.SDcols = c("BFI_N1", "BFI_N2")]

db [, BFI_E1 := 6- BFI_E1r]
db[, extraversion := rowMeans(.SD, na.rm = TRUE),
   .SDcols = c("BFI_E1", "BFI_E2")]

db [, BFI_O1 := 6- BFI_O1r]
db[, openness := rowMeans(.SD, na.rm = TRUE),
   .SDcols = c("BFI_O1", "BFI_O2")]

db [, BFI_C1 := 6- BFI_C1r]
db[, conscientiousness := rowMeans(.SD, na.rm = TRUE),
   .SDcols = c("BFI_C1", "BFI_C2")]

db [, BFI_A2 := 6- BFI_A2r]
db[, agreeableness := rowMeans(.SD, na.rm = TRUE),
   .SDcols = c("BFI_A1", "BFI_A2")]

## create some binary variables (based on the median - will see another way to create based on median later)
db[, StressHigh := factor(Stress >= 10, levels = c(TRUE, FALSE), labels = c("High Stress","Low Stress"))]
db[, SelfesteemHigh := factor(selfesteem >= 3.8, levels = c(TRUE, FALSE), labels = c("High SE", "Low SE"))]

```

# All Categorical Variables

If we are working with all categorical variables, a common way to
present them is to make one variable "continuous" by calculating the
percentages. For example, suppose that in our baseline data collection
exercise data, we wanted to graph the association between categorical
self-esteem and sex 

```{r}

egltable("SelfesteemHigh", g = "sex", data = db)

```

## Bar Plot

We can get a frequency table easily enough (as shown in the earlier code),
but what if we wanted to graph it?

We could graph the frequencies, such as with a bar plot.

```{r}

p.bar <- ggplot(db, aes(sex, fill = SelfesteemHigh)) +
  geom_bar(position = "dodge")

print(p.bar)

```

Beyond the data to ink ratio issues, if we were presenting this or
putting it into a paper, we would want to label it more cleanly.
Here we specify specific breaks on the x axis and specify their labels
and then remove the axis title, since saying male and female makes it
clear enough we don't need to say "sex" the variable name anymore.
We could relabel the y axis (although count is fairly clear, I wanted
to show how to do it). The theme cleans up and makes the font a bit
bigger. 


```{r}

p.bar2 <- p.bar  +
  scale_x_continuous(
    breaks = c(1, 2),
    labels = c("Male", "Female")) +
  xlab("") +
  ylab("Frequency") +
  theme_pubr()

print(p.bar2)
 
```

`r margin_note("You can find more about customizing guides here: https://ggplot2.tidyverse.org/reference/guide_legend.html")`

We can change `SelfesteemHigh` by using 
`scale_fill_manual()` which lets us name the title of the legend and
to specify the colours, by name or hexademical codes, for each group
to make it black and white. We use the `coord_cartesian()` function to
stop the axis expansion so that it begins exactly at zero.
Finally, we get a title, with math symbols by using the `ggtitle()`
function listing the chi-square p-value from `egltable()` analysis
earlier. 

```{r}

p.bar3 <- p.bar2 +
  scale_fill_manual(
    "Self Esteem Group",
    values = c(
      "High SE" = "black",
      "Low SE" = "grey50")) +
  coord_cartesian(expand=FALSE) +
  ggtitle(expression(chi^2~p==.57))

print(p.bar3)

```

## Percentage Plot

A simple way would be to calculate the percentage of `sex = 2` in
each self esteem category and plot that. We would create a new dataset with
percentages calculated along with confidence intervals using the
following code. We calculate the average number of `sex == 2` which
is the proportion, and then use the `prop.test()` function which takes
the count in one group and the total count and can calculate
confidence intervals to get 95% confidence intervals for the proportions.

```{r}

propdata <- db[!is.na(SelfesteemHigh), .(
  Percent = mean(sex == 2, na.rm = TRUE),
  LL = prop.test(
    x = sum(sex == 2, na.rm = TRUE),
    n = sum(!is.na(sex)), correct = FALSE)$conf.int[1],
  UL = prop.test(
    x = sum(sex == 2, na.rm = TRUE),
    n = sum(!is.na(sex)), correct = FALSE)$conf.int[2]),
  by = SelfesteemHigh]

print(propdata)

```

Now we can plot the results. All we need for a basic plot is the
`ggplot()` and `geom_pointrange()` but the rest helps polish up the
figure for presentation.

```{r}

p.prop1 <- ggplot(propdata, aes(SelfesteemHigh, y = Percent, ymin = LL, ymax = UL)) +
  geom_pointrange() +
  scale_y_continuous(labels = percent) +
  scale_x_discrete(
    breaks = c("High SE", "Low SE"),
    labels = c("High SE (median)", "Low SE (median)")) + 
  xlab("") + ylab("Percent Female (95% CI)") + 
 theme_pubr()

print(p.prop1)

```

The same basic strategy can work for many variables at scale fairly
easily. For example, suppose that the personality dimensions were all
categorical. We first create these categorical variables, noting that
this is solely for the sake of demonstration. In general it is not a
good idea to convert continuous variables to categorical ones.

Next, we select just the variables we want (personality, ID, and age)
and reshape the dataset long where each variable is a
"timepoint". This allows us to have data table calculate the
proportions of each easily by setting by self esteem category and by
personality variable. The resulting dataset has proportion and
confidence intervals of high on each personality measure for each self
esteem group. 

```{r}

db[, O := as.integer(openness > median(openness, na.rm=TRUE))]
db[, C := as.integer(conscientiousness > median(conscientiousness, na.rm=TRUE))]
db[, E := as.integer(extraversion > median(extraversion, na.rm=TRUE))]
db[, A := as.integer(agreeableness > median(agreeableness, na.rm=TRUE))]
db[, N := as.integer(neuroticism > median(neuroticism, na.rm=TRUE))]

# Recall the content on reshaping from wide to long format from Lecture 2 (WorkData.rmd) 
dblong <- reshape(
  db[!is.na(SelfesteemHigh), .(ID, SelfesteemHigh, O, C, E, A, N)],
  varying = list(Score = c("O", "C", "E", "A", "N")),
  v.names = "Score",
  timevar = "Personality",
  times = c("O", "C", "E", "A", "N"),
  idvar = "ID",
  direction = "long")

propdata2 <- dblong[, .(
  Percent = mean(Score == 1, na.rm = TRUE),
  LL = prop.test(
    x = sum(Score == 1, na.rm = TRUE),
    n = sum(!is.na(Score)), correct = FALSE)$conf.int[1],
  UL = prop.test(
    x = sum(Score == 1, na.rm = TRUE),
    n = sum(!is.na(Score)), correct = FALSE)$conf.int[2]),
  by = .(SelfesteemHigh, Personality)]

propdata2[, Personality := factor(Personality,
   levels = c("O", "C", "E", "A", "N"))]

print(propdata2)

``` 

Now we can plot the results. All we need for a basic plot is the
`ggplot()`, `geom_pointrange()`, and `facet_grid()` but the rest helps
polish up the figure for presentation. Note that we have not seen 
`facet_grid()` before. Facetting is an idea in data visualizing of
making "small multiples". Essentially the same plot over and over but
with some changes. In this case, its the same plot over and over but
changing the personality measure.

```{r}

p.prop2 <- ggplot(propdata2, aes(SelfesteemHigh, y = Percent, ymin = LL, ymax = UL)) +
  geom_pointrange() +
  scale_y_continuous(labels = percent) +
  scale_x_discrete(
    breaks = c("High SE", "Low SE"),
    labels = c("High SE (median)", "Low SE (median)")) + 
  xlab("") + ylab("Percent High (95% CI)") + 
  theme_pubr() +
  facet_grid(Personality ~ .) +
  coord_flip()

print(p.prop2)

```

# All Continuous Variables

For all continuous variables, there are not many graphing options. A
scatter plot is the main way two continuous variables are visualized.
However, even with a scatter plot, we can add additional
information to make it more useful. Our starting point is the scatter
plots with axes based on five number summaries we saw in Data
Visualization 1 topic. To that we add a linear regression line to help
show the overall association between the two variables. We also add a
text annotation with the correlation coefficient and p-value, found
from running the `cor.test()` function. We label it using `xlab()` and
`ylab()`. The main plot is saved in an object, `p.ss`.
Finally, we use the `ggMarginal()` function from the `ggExtra` package
to add histograms of the univariate distributions to the margins.
The final result captures extensive information about the individual
variables (through the histograms and five number summaries in the
axes) and about their association (through the scatter plot,
regression line, and correlation coefficient).


```{r, fig.height = 5.5, fig.width = 5.5, fig.cap = "scatter plot with regression line and correlation"}

cor.test(~ selfesteem + Stress, data = db)

p.ss <- ggplot(db, aes(Stress, selfesteem)) +
  geom_point() +
  stat_smooth(method = "lm", se = FALSE, size = 1) + 
  scale_x_continuous(breaks = as.numeric(quantile(db$Stress))) + 
  scale_y_continuous(breaks = as.numeric(quantile(db$selfesteem))) +   
  theme_pubr() +   
  theme(axis.line = element_blank()) +
  geom_rangeframe() +
  xlab("Perceived Stress") +
  ylab("Self Esteem") +
  annotate("text", x = max(db$Stress), y = max(db$selfesteem),
           label = "r = -0.65, p < .001",
           size = 6, hjust = 1, vjust = 1)

## now add a histogram to the margins
ggMarginal(p.ss, type = "histogram")

``` 

`r margin_note("Because we did not store the results of Stress
and selfesteem with the histograms added to the margins, we have the
basic scatter plots in the figure, but we could have the histograms as
well if desired by saving the result in p.ss.")`

Another feature that is helpful with figures is to arrange sets of
related figures together. For example, in the following code we make a
plot of Stress and neuroticism scores and save it in `p.sn`.
Now we can make a panel of graphs with two columns using the
`ggarrange()` function. 

```{r, fig.height = 5, fig.width = 10, fig.cap = "panel graph of two scatter plots"}

cor.test(~ neuroticism + Stress, data = db)

# note I had to add na.rm argument to the quantile and min functions for neuroticism
p.sn <- ggplot(db, aes(Stress, neuroticism)) +
  geom_point() +
  stat_smooth(method = "lm", se = FALSE, size = 1) + 
  scale_x_continuous(breaks = as.numeric(quantile(db$Stress))) + 
  scale_y_continuous(breaks = as.numeric(quantile(db$neuroticism, na.rm = TRUE))) +   
  theme_pubr() +   
  theme(axis.line = element_blank()) +
  geom_rangeframe() +
  xlab("Perceived Stress") +
  ylab("Neuroticism") +
  annotate("text", x = max(db$Stress), y = min(db$neuroticism, na.rm = TRUE),
           label = "r = 0.49, p < .001",
           size = 6, hjust = 1, vjust = 0)

ggarrange(
  p.ss, p.sn,
  ncol = 2,
  labels = c("A", "B"))

``` 

If we had more than two continuous variables we wanted to plot, there
are fewer options. We could create a 3D graph, but those rarely work
well for publications or theses that are printed and only viewed in
2D. Instead, its common to map additional variables to other aspects
or aesthetics in the figure. For example we can take our scatter plot
of Stress and self esteem and make the size and shading of points
proportional to neuroticism scores. The following code does this and
plots the result. The only additions are adding
`size = neuroticism` and `colour = neuroticism` to 
`geom_point()`.

```{r, fig.height = 5.5, fig.width = 5.5, fig.cap = "scatter plot with regression line and correlation of stress, self esteem and neuroticism"}

cor.test(~ selfesteem + Stress, data = db)

p.ssn <- ggplot(db, aes(Stress, selfesteem)) +
  geom_point(aes(size = neuroticism, colour = neuroticism)) +
  stat_smooth(method = "lm", se = FALSE, size = 1) + 
  scale_x_continuous(breaks = as.numeric(quantile(db$Stress))) + 
  scale_y_continuous(breaks = as.numeric(quantile(db$selfesteem))) +   
  theme_pubr() +   
  theme(axis.line = element_blank()) +  
  geom_rangeframe() +
  xlab("Perceived Stress") +
  ylab("Self Esteem") +
  annotate("text", x = max(db$Stress), y = max(db$selfesteem),
           label = "r = -0.65, p < .001",
           size = 6, hjust = 1, vjust = 1)

print(p.ssn)

``` 

By default there are separate legend guides, one for size and one for
the colour. We can customize this using the `guides()` function in
`ggplot2`. We could turn one off or by using the same title make them
the same. Where we wrote `Neuroticism` capitalized, we also could have
changed the title of the legend guide (anything within the quotes
would be valid).

```{r, fig.height = 5.5, fig.width = 5.5, fig.cap = "scatter plot with regression line and correlation of stress, self esteem and neuroticism with one guide"}

p.ssn + 
  guides(
    size = guide_legend("Neuroticism"),
    colour = guide_legend("Neuroticism"))

``` 

# Mixed Continuous and Categorical Variables

The most variety of graphs are possible with data that are a mix of
continuous and categorical variables. As an example we will work with
self esteem group, sex, and the five personality traits. We start by
reshaping them long so that which personality measure is being
examined becomes another categorical variable.

```{r}

dblong2 <- reshape(
  db[!is.na(SelfesteemHigh), .(ID, SelfesteemHigh, sex, openness, conscientiousness, extraversion, agreeableness, neuroticism)],
  varying = list(Score = c("openness", "conscientiousness", "extraversion", "agreeableness", "neuroticism")),
  v.names = "Score",
  timevar = "Personality",
  times = c("O", "C", "E", "A", "N"),
  idvar = "ID",
  direction = "long")
dblong2[, Personality := factor(Personality, levels = c("O", "C", "E", "A", "N"))]

head(dblong2)

``` 

We can make a simple plot with the means and 95% confidence intervals
using the following code.

```{r}

p.mean1 <- ggplot(dblong2, aes(Personality, Score)) +
  stat_summary(fun.data = mean_cl_normal) +
  theme_pubr()

print(p.mean1)

``` 

There are lots of additions we could consider to this simple
figure. The labels are short, but not the most informative. 
We could re-label the axis.

```{r}

p.mean2 <- p.mean1 +
  scale_x_discrete("",
    breaks = c("O", "C", "E", "A", "N"),
    labels = c("Openness", "Conscientiousness", "Extraversion", "Agreeableness", "Neuroticism"))

print(p.mean2)

```

Long labels are messy in smaller spaces. We could rotate the labels to
make space or rotate the graph.

```{r}

ggarrange(
  p.mean2 +
    theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1)) +
    ggtitle("rotate text"),
  p.mean2 +
    coord_flip() +
    ggtitle("rotate graph"),
  ncol = 1)

```

Either rotating the labels or the graph made it easier to have long
labels and clearly read them, in this case especially rotating the
graph so the longest words can be read in their usual left to right
orientation.

With smaller datasets, we could show raw data as well as the means.
Although relatively easy to add, the result is disastrous. Even though
this is not a huge dataset, there is enough data and because there are
not many different possible values for each personality measure, the
dotplot is very difficult to read.
Lastly, because of the black dots and the means being shown as black
dots, it becomes impossible to well see the means.

```{r}

p.mean2 +
  geom_dotplot(binaxis = "y", stackdir = "center", binwidth = .2) + 
    coord_flip() +
    ggtitle("rotate graph")

```
By shrinking the size of the dots further (using `binwidth = .1`),
adding some transparency using the `alpha = .2` argument (valid
numbers are 0 completely transparent to 1 completely opaque) and
adding some random noise on the scores using `jitter()` we can see the
raw data and means better. It is not quite the actual raw data,
because we have added some noise, but it could still help to show the
general spread of data. 

```{r}

p.mean2 +
  geom_dotplot(aes(y = jitter(Score, 2)), binaxis = "y",
               alpha = .2,
               stackdir = "center", binwidth = .1) + 
    coord_flip()

```

For larger datasets or if the dotplot with noise is not as useful
anymore because it's not true raw data, a more summarized version of the
distribution can be shown with a violin plot, which is basically a
density plot that is mirrored. Thicker regions have more points,
narrow regions have fewer data points. We can also see the
range/spread of each variable and still have our mean and confidence
interval summaries shown clearly.

```{r}

p.mean3 <- p.mean2 +
  geom_violin(fill = NA) + 
  coord_flip()

print(p.mean3)

```

Another aspect we could imporve: there is no necessary ordering of
personality measures. Ordering them such as from highest to lowest
mean can be used to help us read the plot more easily.

We do this by changing the levels of the factor in the dataset.
First, we calculate the mean score by personality, then we have data
table order the resulting means from highest to lowest (these are
things we saw in the working with data topic).
Finally, we use `factor()` on personality and specify the levels in
this same order.

```{r}

dblong2[, .(M = mean(Score, na.rm = TRUE)), by = Personality][
  order(-M)]

dblong2[, Personality := factor(Personality,
                                levels = c("C", "O", "A", "N", "E"))]

```

Now we can simply remake our graph (note that this only works when
using data.table for data management, if using data frames etc. you
would want to copy and paste all your graph code again). 

By having ordered the variables by their means, it helps us rapidly
interpret which one has the lowest score (extraversion) and which the
highest average score (conscientiousness). It is a small step but one
that aids rapid processing of the figure and the data therein.

```{r}

print(p.mean3)

```

It is easy to add additional categorical variables into a figure.
For example, we can colour by self esteem group.

```{r}

ggplot(dblong2, aes(Personality, Score, colour = SelfesteemHigh)) +
  stat_summary(fun.data = mean_cl_normal, position = position_dodge(.2)) +
  scale_x_discrete("",
    breaks = c("O", "C", "E", "A", "N"),
    labels = c("Openness", "Conscientiousness", "Extraversion", "Agreeableness", "Neuroticism")) + 
  theme_pubr() +
  coord_flip()

```

If we wanted we could also add shapes by `sex`. 
In this example, I also change the default colour and legend title for
`SelfesteemHigh` to SE Groups.

```{r}
    
ggplot(dblong2, aes(Personality, Score, colour = SelfesteemHigh, shape = factor(sex))) +
  stat_summary(fun.data = mean_cl_normal, position = position_dodge(.3)) +
  scale_x_discrete("",
    breaks = c("O", "C", "E", "A", "N"),
    labels = c("Openness", "Conscientiousness", "Extraversion", "Agreeableness", "Neuroticism")) + 
  theme_pubr() +
  coord_flip() +
  scale_colour_manual(
    "Self Esteem Group",
    values = c("High SE" = "black", "Low SE" = "grey80"))

``` 

If having four means side by side is too hard to read, we could facet
the plot into small multiples, say by sex, so that we can compare
age groups in male and female participants.

```{r, fig.width = 7, fig.height = 10}

# note the use of na.omit at the beginning of ggplot. This is because there is NA data in 'sex'
# and we don't want a separate facet for sex = NA
ggplot(na.omit(dblong2), aes(Personality, Score, colour = SelfesteemHigh)) +
  stat_summary(fun.data = mean_cl_normal, position = position_dodge(.3)) +
  scale_x_discrete("",
    breaks = c("O", "C", "E", "A", "N"),
    labels = c("Openness", "Conscientiousness", "Extraversion", "Agreeableness", "Neuroticism")) + 
  theme_pubr() +
  coord_flip() + 
  facet_grid(sex ~ .) +
  scale_colour_manual(
    "Self Esteem Group",
    values = c("High SE" = "black", "Low SE" = "grey80"))

```

If we wanted to show the raw data, we could facet on both self esteem and
sex and add our dotplots back in.

```{r, fig.width = 10, fig.height = 9}

ggplot(na.omit(dblong2), aes(Personality, Score)) +
  geom_dotplot(aes(y = jitter(Score, 2)), binaxis = "y",
               alpha = .2,
               stackdir = "center", binwidth = .1) +   
  stat_summary(fun.data = mean_cl_normal, position = position_dodge(.2)) +
  scale_x_discrete("",
    breaks = c("O", "C", "E", "A", "N"),
    labels = c("Openness", "Conscientiousness", "Extraversion", "Agreeableness", "Neuroticism")) + 
  theme_pubr() +
  coord_flip() + 
  facet_grid(sex ~ SelfesteemHigh)

```

When we facet, only the labels show up making it difficult to
interpret if this was being presented in presentation or article. In
this case, we might create new variables with more descriptive labels,
just for the plotting. Note that the choice of jitter, alpha, and
binwidth all involve some trial and error to get to a plot that is
easy to read and visually appealing (admittedly, a rather subjective
concept). 

```{r, fig.width = 10, fig.height = 9}

dblong2[, Sex2 := factor(sex, levels = c(1, 2), labels = c("Male", "Female"))] # we already made a factor for SE

ggplot(na.omit(dblong2), aes(Personality, Score)) +
  geom_dotplot(aes(y = jitter(Score, 2)), binaxis = "y",
               alpha = .2,
               stackdir = "center", binwidth = .1) +   
  stat_summary(fun.data = mean_cl_normal, position = position_dodge(.2)) +
  scale_x_discrete("",
    breaks = c("O", "C", "E", "A", "N"),
    labels = c("Openness", "Conscientiousness", "Extraversion", "Agreeableness", "Neuroticism")) + 
  theme_pubr() +
  coord_flip() + 
  facet_grid(Sex2 ~ SelfesteemHigh)

```

Next, we are going to look at some hypothetical data from an
intervention comparing augmented Treatment as Usual (TAU+) to
Cognitive Behavioural Therapy (CBT+). The two conditions are measured
at baseline and post intervention on depression symptoms.
The first part of the code just simulates some data including a wide
dataset, `trial` and a long dataset, `trial2`.
**You do not need to follow this code, it is just to get us some
sample data to work with.**

```{r}

## code to make an example dataset
set.seed(1234)
trial <- data.table(
  ID = sample(1:70),
  Group = factor(rep(c("TAU+", "CBT+"), each = 35)),
  B_Dep = pmax(round(rnorm(35*2, mean = 22, sd = 7)), 0))
trial[, P_Dep := round(B_Dep * rnorm(70, mean = ifelse(Group == "CBT+", .5, .9), sd = .2))]
trial2 <- reshape(trial, varying = list(c("B_Dep", "P_Dep")), v.names = "Depression",
                 timevar = "Assessment", times = c(0, 1),
                 idvar = "ID", direction = "long")

head(trial)

head(trial2)

```

With some sample data, we can plot the long dataset to show the mean
and confidence intervals for each group at each time point.

```{r}

p.trial1 <- ggplot(trial2, aes(Assessment, Depression, colour = Group)) +
  stat_summary(fun.data = mean_cl_normal,
               position = position_dodge(.05),
               geom = "pointrange") +
  theme_pubr()
p.trial1 <- set_palette(p.trial1, palette = "jco")

print(p.trial1)

```

Because these are longitudinal data, it makes sense to connect them
with lines to show how they changed over time. We do this by adding a
line geom based on the mean. Then we tidy up the x axis labels and the
y axis labels.

Finally, something new, we use `geom_hline()` to add a horizontal line
at 16, a common cut off on the CES-D indicative of clinically
significant depression symptoms. We make this a dashed, grey line to
make it less prominent. This line aids interpretation by helping
people anchor the results to common cut offs. We also use the 
`coord_cartesian()` function to change the limits of the graph. Since
the CES-D scale starts at 0 (meaning lowest possilbe / no depression
symptoms) we make that the y axis limit. The x axis limits are based
on the coding of assessments and the upper y axis limit we base
visually off the upper confidence interval.

```{r}

p.trial1b <- p.trial1 + 
  stat_summary(fun = mean,
               position = position_dodge(.05),
               geom = "line") +
  scale_x_continuous("",
                     breaks = c(0, 1),
                     labels = c("Baseline", "Post")) +
  scale_y_continuous("Depression Symptoms (CES-D)",
                     breaks = c(0, 4, 8, 12, 16, 20, 24)) + 
  geom_hline(yintercept = 16, linetype = 2, colour = "grey50") +
  coord_cartesian(xlim = c(-.05, 1.05), ylim = c(0, 26.5), expand = FALSE)

print(p.trial1b)

```

The other information that would be useful would be to annotate with
information about group differences and change over time.
First we run a regression on depression by group at each time point
and then use those p-values to add annotations to the graph.

```{r}

summary(lm(Depression ~ Group,
        data = trial2[Assessment == 0]))

summary(lm(Depression ~ Group,
        data = trial2[Assessment == 1]))

p.trial1b +
  annotate("text", x = 0, y = 26, label = "italic(n.s.)", parse = TRUE) + 
  annotate("text", x = 1, y = 26, label = "***")

```

In smaller datasets we could visualize the individual changes in
depression symptoms. We again plot depression symptoms on the y axis,
assessment on the x axis and colour by group, but instead of
summarizing the data, we directly plot points and lines. We use the
`group = ID` to indicate we want a different line for each ID in the dataset.

```{r}

p.trial2 <- ggplot(trial2, aes(Assessment, Depression, colour = Group, group = ID)) +
  geom_line() +
  geom_point() + 
  scale_x_continuous("",
                     breaks = c(0, 1),
                     labels = c("Baseline", "Post")) +
  scale_y_continuous("Depression Symptoms (CES-D)") + 
  geom_hline(yintercept = 16, linetype = 2, colour = "grey50") +
  theme_pubr()
p.trial2 <- set_palette(p.trial2, "jco")

print(p.trial2)

```

The result lets us see the starting point and change over time for
each person, but its a bit messy. Rather than just colour by group, it
might be helpful to separate by group, which we do by facetting.

```{r, fig.width = 9, fig.height = 6}

p.trial2 + facet_grid(. ~ Group)

```

That worked, but now our labels overlap. We need to add some space
between each facet (panel). Since each panel is labelled, we do not
really need the legend guide for group, so we turn that off by using
the `guides()` function to clean the plot up a bit.

```{r, fig.width = 9, fig.height = 6}

p.trial2 + facet_grid(. ~ Group) +
  theme(panel.spacing = unit(2, "lines")) +
  guides(colour = "none")

```

Another way to show individual change would be to use the wide dataset
to calculate individual change scores. A common approach is to examine
the percent change. We subtract 1 so that 0 means no change.
To plot the results, we put the individual IDs on the x axis and the
height of the bars is the percent change.

```{r}

trial[, PercentChange := P_Dep/B_Dep - 1]

p.trial3 <- ggplot(trial, aes(ID, PercentChange, fill = Group)) +
  geom_bar(stat = "identity") +
  theme_pubr() +
  scale_y_continuous("Change from Baseline", labels = percent)

p.trial3 <- set_palette(p.trial3, "jco")

print(p.trial3)

``` 

Although this figure is technically accurate, it is difficult to
interpret. The general pattern seems to be that the CBT+ group has a
more negative change. Ordering the data can improve this.
We use the `order()` function to order by percent change and then
order that to get numbers for a "new" ID variable. Now we can remake
the plot, with a few other tweaks to clean it up (a line at 0, no
change, removing the x axis and adding a better x axis title).

```{r}

trial[, ID2 := order(order(PercentChange))]

p.trial4 <- ggplot(trial, aes(ID2, PercentChange, fill = Group)) +
  geom_hline(yintercept = 0) + 
  geom_bar(stat = "identity") +
  theme_pubr() +
  scale_y_continuous("Change from Baseline", labels = percent) +
  xlab("Individual Participants") + 
  theme(
    axis.line.x = element_blank(),
    axis.ticks.x = element_blank(),
    axis.text.x = element_blank())

p.trial4 <- set_palette(p.trial4, "jco")

print(p.trial4)

```

With the data ordered, it is much easier to see the biggest decline,
the biggest increase and to see that the CBT+ group dominates the left
hand side with the largest decreases while little decrease or even
increases occur almost exclusively in the TAU+ group.

If we wanted a slight modification is to order first by group and then
by percent change, giving the followiing result.

```{r}

trial[, ID3 := order(order(Group, PercentChange))]

p.trial5 <- ggplot(trial, aes(ID3, PercentChange, fill = Group)) +
  geom_hline(yintercept = 0) + 
  geom_bar(stat = "identity") +
  theme_pubr() +
  scale_y_continuous("Change from Baseline", labels = percent) +
  xlab("Individual Participants") + 
  theme(
    axis.line.x = element_blank(),
    axis.ticks.x = element_blank(),
    axis.text.x = element_blank())

p.trial5 <- set_palette(p.trial5, "jco")

print(p.trial5)

```

# Summary Table

Here is a little summary of some of the functions used in this
topic. You might also enjoy this "cheatsheet" for `ggplot2`:
https://github.com/rstudio/cheatsheets/raw/master/data-visualization-2.1.pdf


| Function       | What it does                                 |
|----------------|----------------------------------------------|
| `ggplot()`     | Sets the dataset and which variables map to which aesthetics for a plot |
| `geom_point()` | Adds points such as for a scatter plot|
| `geom_hline()` | Adds a horizontal line at a specific y axis value |
| `stat_summary()` | Used to automatically calculate some summary statistics on data and plot, usually means with standard errors or confidence intervals | 
| `stat_smooth()` | Used to automatically calculate a regression line | 
| `ylab()` | Adds a label for the y axis |
| `xlab()` | Adds a label for the x axis |
| `theme_pubr()` | A cleaner black and white theme for `ggplot2` |

